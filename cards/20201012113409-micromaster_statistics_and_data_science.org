#+TITLE: Micromaster Statistics and Data Science
#+OPTIONS: toc:nil
#+ROAM_ALIAS: micromaster-statistics-and-data-science
#+ROAM_TAGS: micromaster-statistics-and-data-science msds edx probability statistics machine-learning
#+DATE: 2020-10-12
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 8pt]
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{multicol}
#+LATEX_HEADER: \usepackage[landscape, margin=0.5cm]{geometry}
#+OPTIONS: title:nil
#+OPTIONS: author:nil
#+OPTIONS: date:nil

# * Probability and Statistics

# \footnotesize

#+begin_export latex
\begin{multicols*}{3}

\paragraph{Partitions}
Given $n$ elements, and $k_i \in \mathbb{N}, i=1, \dots, r$ with $\sum_i k_i = n$, the number
of partition of the set into $r$ disjoint subset with the $i^{th}$ subset
containing exactly $k_i$ elements is equal to
\begin{align*}
  \binom{n}{k_1 \cdots k_r} = \frac{n!}{k_1!\dots k_r!}.
\end{align*}

\paragraph{Transformation}
Let $X$ be a continuous r.v and $g$ monotonoic when $f_X(x) > 0$ then for
$Y=g(X)$, $f_y(y) = f_X(h(y))\vert \partial_yh(y)\vert$, where $h = g^{-1}$ where $g$
is monotonic. If $Z=X+Y$ then $f_z(z) = \int f_X(x)f_Y(z-x) dz$.

\paragraph{Expectation}
\begin{align*}
Eg(\vec X) & = \sum_{\vec x} g(\vec x) p_{\vec X}(\vec x), \quad g: \mathbb{R}^n \to \mathbb{R} \\
Eg(\vec X) & = \int_{\mathbb{R}^n} g(\vec x) f_{\vec X}(\vec x) d\vec x
\end{align*}

\paragraph{Dependence}
$Cov(X,Y)$ is bilinear, and $Cov(X,Y) = E[(X-\mu)(Y-\nu)] = EXY -
EXEY$. $\rho_{X,Y}=Cov(x,y)/\sigma_x\sigma_y$ The total variance is
$Var(Y) = E[Var(Y\vert X)] + Var(E[Y\vert X])$. If $N$ random,
$X_1, \dots, X_N$ are iid, then
\begin{align*}
  EY & = EN \cdot EX \\
  Var(Y) & = EN \cdot Var(X) + (EX)^2 Var(N)
\end{align*}

Iterated expectetation $E[E[X \vert Y] = E[X]$. $E[X\vert Y]$ is the rv that
take value $E[X \vert Y = y]$ whenever $Y=y$.

\paragraph{Conditional PMF}
\begin{align*}
p_{X\vert Y}(x \vert y) & = \frac{p_{X,Y}(x,y)}{p_Y(y)} \\
f_{X\vert Y}(x \vert y) & = \frac{f_{X,Y}{X,Y}(x,y)}{f_Y(y)}
\end{align*}

\paragraph{Markov-chain}
\emph{recurrent} (come back), \emph{transient} (never come back),
GCD of number of steps for returning is $ >1 $, otherwise \emph{aperiodic}.
\emph{Transition matrix} $Q^1$, where $q_{ij} = P(X_{n+1}=i\vert X_n =
i)$. $Q^m$ gives the probability for $m$ steps. If $X_0 \sim \vec p$, then
$X_n \sim \vec{p} Q^n$. \emph{Stationary} is $\vec sQ =  \vec s$.


\paragraph{Inequalities}
\begin{enumerate}
\item $\vert EXY \vert^2 \leq EX^2 EY^2$ (Cauchy-Schwarz)
\item $P(X \geq a) \leq E\vert X \vert / a$, $a> 0$. (Markov)
\item $P(\vert X-\mu \vert \geq a) \leq \sigma^2/a^2$. (Chebyshev)
\item $g(EX) \leq Eg(X)$ if $g$ convex, reverse if $g$ concave. (Jensen)
\item $P(\vert X-\mu \vert \geq \epsilon) \leq 2exp(-2n\epsilon^2/(b-a)^2)$, $X
  \in [a,b]$, $\forall \epsilon >0$. Replace with $\epsilon = c/\sqrt(n)$ (Hoeffding).
\end{enumerate}

\paragraph{Convergence}
$(Y_n)_{n=1}^\infty \to Y$ in probability if
$P(\vert Y_n - Y \vert < \epsilon) \to 1$, $n \to \infty$,
$\forall \epsilon > 0$, $Y$ a rv. If $X_n \to a$, $Y_n \to b$
in prob. (constants), then $X_n+Y_n \to a+b$, if $g\in C^0(\mathbb{R})$,
$g(X_n) \to g(a)$. $EX_n$ does not always converge to $a$.

WLLN. $(X_i)_{i=1}^\infty$ i.i.d, $EX_i = \mu$,
$X \in L^2(\mathbb{R})$, then $\forall \epsilon > 0$
\begin{align*}
\lim_{n\to\infty} P(\vert \bar X_n - \mu \vert < \epsilon) = 1
\end{align*}


CLT. Same assumptions as wlln. Then $Var(X_i) = \sigma^2$, and
$Z_n = \sum_{i=1}^n (X_i-\mu)/(\sigma\sqrt n)$, then
\begin{align*}
\lim_{n\to\infty} F_{Z_n}(z)\to F_z(z), \quad Z \sim \mathcal{N}(0, 1).
\end{align*}


De Moivre-Laplace Approximation to Binomial
\begin{align*}
  P(X=i) = P\Big(i-\frac{1}{2} \leq X \leq i + \frac{1}{2}\Big)
\end{align*}
using CLT to approximate the PMF of $X$.

Almost surely $T_n \to T$ \emph{a.s} if $P(\{\omega: T_n(\omega) \to T(w), n \to \infty\}) = 1$.

Convergence in distribution: $T_n \to T$ in $(d)$, if $F_{T_n}(z) \to
F_T(z)$ for all $z$ that are continuous, equivalently $E[f(T_n)] \to E[f(T)]$
for all continuous bounded function $f$.

Properties: $(T_n)_{n\geq 1}$ converge $a.s$, $\Rightarrow$ in $P$, and limit
are equal a.s. Convergence in $P$ implies convergence in $(d)$. Convergence in
distribution implies convergence of probability if the limit has a density.

Linear and multiplication and division holds in the limit for a.s. conv and
prob. conv. (division, denominator is not 0). \emph{Slutsky}: if $T_n \to T$ in
$(d)$, and $U_n \to u$ in $P$, and $u$ constant, then $T_n + U_n \to T + u$ in
$(d)$, $T_nU_n \to Tu$ in $(d)$, and $u\neq0$, $T_n/U_n \to T/u$ in $(d)$.

Continuous mapping theorem: for all type of convergence, $T_n \to T \Rightarrow
f(T_n) \to f(T)$, when $f \in C^0(\mathbb{R})$.

\paragraph{Laws}

Bernouilli: $p(k)=\binom{n}{k} p^k (1-p)^{n-k}$, $EX=p$, $Var(X)=np(1-p)$.

Geometric: $p(k) = (1-p)^{k-1}p$, $EX = p^{-1}$, $Var(X)= (1-p)/p^2$.  Let $n$
be a given a time and let $T$ be the first time of success after $n$. Then
$T-n$ follows a geometric distribution with parameter $p$ and
$T - n \perp X_1, \dots,X_n$. Let $Y_k = \sum_i=1^k T_k$, then $EY_k=k/p$,
$Var(Y_k)=kp/(1-p)^2$. PMF is Pascal PMF of order $k$
\begin{align*}
  p_{Y_k}(t) = \binom{t-1}{k-1}p^k (1-p)^{t-k}, \quad t=k,k+1, \dots
\end{align*}

Poisson: $p(k) = e^{-\lambda} \lambda^k/k!$, $k=0,1,\dots$. $EX=\lambda$,
$Var(X)=\lambda$. For a fixed lambda, binomial law converge to poisson with
$p=\lambda/n$. So poisson is a good approx if $\lambda = np$, $n$ large and $p$
really small.

Poisson process $P(k, \tau)$ ($k$ arrivals, intervals length $\tau$):
time-homogeneity, independence, small interval probabilities (probabilities are
$O(\tau)$.

Exponential: $f(t) = \lambda \exp(-\lambda t)$, $EX = 1/\lambda$,
$Var(X)=1/\lambda^2$.

Poisson is indpendent of paste, waiting time is exponential and indpendent of
past. $k^{th}$ arrival time is described as the sum of each arrival time and
has $EY_k=k/\lambda$ and $Var(Y_k) = k/\lambda^2$. PDF is
$f(y) = \lambda^k y^{k-1} e^{-\lambda y}/(k-1)!$.

\paragraph{Sum of RV}
If $Y = X_1 + \dots + X_N$, then
\begin{enumerate}
\item $X_i \sim Ber(p)$, $N \sim Bin(m, q)$, $Y\sim Bin(m, pq)$.
\item $X_i \sim Ber(p)$, $N \sim Poi(\lambda)$, $Y\sim Poi(\lambda p)$.
\item $X_i \sim Geom(p)$, $N \sim Geom(q)$, $Y\sim Geom(pq)$.
\item $X_i \sim Exp(\lambda)$, $N \sim Geom(q)$, $Y\sim Exp(\lambda q)$.
\end{enumerate}

When two process are poissons, the distribution of the combination of both
event is a poisson where rates are added ($\lambda = \sum_i r_i$).. Expectation
time between two events is twice the mean. Each event in a component has a
probability $r_i/\lambda$.

\paragraph{Bayesian Inference} Maximum a posteriori prob. (MAP), Least mean
squares (LMS), Linear least mean squares (OLS). Bayesian inference:
\begin{enumerate}
\item start with a prior $f_\Theta$ of $\Theta$.
\item have a model $f_{X\vert \Theta}$ of the observation $X$.
\item Update $p_{\Theta\vert x}$ using Baye's rule.
\end{enumerate}
Update rule (adapt for discrete laws).
\begin{align*}
  f_{\Theta \vert X}(\theta \vert x) =
  \frac{f_\Theta(\theta) f_{X \vert \Theta}(x \vert \theta)}
  {\int f_\Theta(\eta) f_{X \vert \Theta}(x \vert \eta) d\eta}
\end{align*}
\textbf{MAP} (\emph{maximum a posteriori}): $\hat\theta = \textrm{argmax}_\theta f_\Theta(\theta)f_{X\vert \Theta}(x\vert
\theta)$ ($\hat\theta$ maximizes the posterior). If $\Theta$ is discrete, the
MAP minimizes (over all decision rules) the prob. of selecting an incorrect
hypothesis. Estimator is a function of the data.  Conditional Expectation (LMS)
sets the $\hat \theta = E[\Theta \vert X = x]$. Hypothesis testing: MAP rules
selects hypothesis which has the largest a posteriori distribution. The MAP
rule minimizes the probability of selecting an incorrect hypothesis for any
observation value x, as well as the probability of error over all decision
rules.

Estimators: $\hat\theta = E\Theta$ minimizes the equation $E(\Theta-\theta)^2$
(idem for $E[\,\cdot\, \vert X = x]$). The \textbf{bayes estimator} $\hat g(X) = E[\Theta \vert X]$ minimizes
$E(\Theta - g(X))^2$ over all estimators. $\hat g(X)$ is unbiased [0
(un)conditional mean]. The error $\tilde \Theta = \hat \Theta - \theta $
is uncorrelated with the estimates. The variance of $\Theta$ can be decomposed as
$Var(\hat \Theta) + Var(\tilde \Theta)$. The linear LMS is given by
\begin{align*}
  \hat \Theta = E \Theta + \frac{cov(\Theta, X)}{var(X)} (X - EX)
\end{align*}
with mse equal to $(1- \rho^2)\sigma_\Theta^2 $,
$\rho = cov(\Theta, X)/(\sigma_\Theta\sigma_X)$.


\paragraph{Classical Statistical Inference}
Estimator error $\tilde \Theta = \hat \Theta - \theta $, bias
$E \tilde \Theta $. Expected value, variance and bias depends on $\hat \Theta$
while estimation error also $X_1 \dots, X_n$. Unbiased if bias is 0 for all
$\theta$, asymptotically unbiased if $E\hat \Theta_n \to \theta$, for all
$\theta$. An estimator is consistent if the sequence $\hat \Theta_n$ converge
to the true parameter $\theta$ for all possible $\theta$.

MLE $\hat \theta = \textrm{argmax}_\theta h(f_X(x \vert \theta))$, where $h$ is bijective function
(e.g. $\log$). Under some condition MLE is consistent and asymptotically
normal. $\bar X_n$ is unbiased for $\mu$ and variance $\sigma^2/n$. Variance
estimator $\hat S^2_n = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X_n)^2$ is
unbiased.

Confidence Interval $\mathcal{I}$ of $1-\alpha$ such that
\begin{align*}
P(\Theta \in [\hat \Theta^-_n, \hat \Theta^+_n])) \geq 1-\alpha, \quad \forall \textrm{ valid } \theta.
\end{align*}
$\mathcal{I}$ is random and the boundaries does not depend on $\Theta$.

OLS: $\beta_1 = \sigma_{xy}/\sigma_x$, $\beta_0 = \bar y - \beta_1 \bar x$,
with $\sigma_x$ begin the biased estimator of the standard deviation. Bayesian LS assume
\begin{itemize}
\item $Y_i = \Theta_0 + \Theta_1 x_i + W_i$
\item $x_i$ are known constants, $\Theta_0, \Theta_1, W_j$ are normal independent.
\item $E\Theta_0 = E\Theta_1 = 0$ and variance $\Theta_i = \sigma_i^2$, $i=1,2$, $W_j \sim \mathcal{N}(0, \sigma^2)$.
\end{itemize}

Estimates are
\begin{align*}
  \hat \theta_1 & = \frac{\sigma_1^2}{\sigma^2+\sigma_1^2\sum_i (x_i - \bar x)Â²}
  \sum_{i=1}^n (x_i -\bar x) (y_i - \bar y) \\
  \hat \theta_0 & = \frac{n\sigma_0^2}{\sigma^2+n\sigma_0^2} (\bar y - \hat \theta_1 \bar x)
\end{align*}

Likelihood ratio test: start with a target value $\alpha$ (5\%) for false
rejection prob. Choose $\xi$ such that $P(L(X)>\xi \vert H_0) = \alpha$. Once
the value $x$ of X is observed, reject $H_0$ is $L(x) > \xi$. Neyman-Person
Lemma, given $\xi$, we have $P(L(X)>\xi \vert H_0) = \alpha$ and
$P(L(X)>\xi \vert H_1) = \beta$. Suppose that some other test, with rejection
region $R$, achieves a smaller or equal false rejection prob:
$P(X\in R \vert H_0) \leq a$. Then $P(X \notin R \vert H_1) \geq \beta$, with
strict inequality, when $P(X \in R \vert H_0) < \alpha$.

Significance testing method: choose a test statistic, find the shape of
rejection region given $H_0$, choose the significance level, and the critical
value $\xi$ so that prob. of false rejection is around $\alpha$. This sets the
rejection region. Reject hypothesis $H_0$ if the observed test statistics falls
in the rejection region.

\paragraph{Gaussian}
Gaussian is symmetric, stable for linear transformation ($\sigma Z + \mu \sim
\mathcal{N}(\mu, \sigma^2), Z \sim \mathcal{N}(0, 1)$. Quantiles $F(q_alpha)
= 1-\alpha$. For $Z \sim \mathcal{N}(0, 1)$, $q_{2.5\%} = 1.96$.

\paragraph{Statistical Model}
\emph{Statistical experiment} is a sample of $X_1, \dots, X_n ~ P$ iid, with
$supp(X) = E \subset \mathbb{R}$. A \emph{stat. model} is the pair $(E,
(P_\theta)_{\theta\in\Theta}))$, where $E$ is the \emph{sample space},
$(P_\theta)_{\theta\in\Theta})$ a family of probability measure on $E$,
$\Theta$ is the parameter set. Well specified means $\exists \theta \vert P =
P_\theta$. $\theta$ is the true \emph{parameter}. When $\Theta \subset
\mathbb{R}^d$, parametric model, when $\theta$ has infinite dimension, then
nonparametric. Semi parametric, when $\Theta$ can be decomposed in two subset,
one of whiche is finite dimension. Parameter $\theta$ is identifiable if the
$\theta: \Theta \to P_\theta$ is injective $(P_\theta = P_\eta \Rightarrow
\theta = \eta).$

A statistic is a function of the data, an esimator of $\theta$ is a statistic
not depending on $\theta$, a weakly consistent $\hat\theta_n$ if $\hat \theta_n
\to \theta$ in $P$ w.r.t $P_\theta$. Strongly consistent if $\to$ is
$.a.s.$. $\hat\theta_n$ is asymptotically normal if $\sqrt n (\hat\theta_n -
\theta) \to \mathbacl{N}(0, \sigma^2)$, where $\sigma^2$ is the asymptotic
variance or $\hat\theta_n$. Quadratic risk $R(\hat\theta_n)= E[(\hat\theta_n-
  \theta)^2]$ which is equal to $Var(\hat\theta_n) + (E[\hat\theta_n] -
\theta)^2$. A CI $\mathcal{I}$ of asymptotic level $1-\alpha$ for $\theta$ if
$\lim_{n\to \infty} P_\theta(\theta \in \mathcal{I}) \geq 1-\alpha$. In
practice, we can bound the variance of the estimator, or solve the
inequalities, or plug-in (replace the parameter in the variance with the
estimate.

\paragraph{Delta $\Delta$ method}.
Let $\sqrt n (Z_n-\theta)/\sigma^2 \to \mathcal{N}(0, 1)$, $g \in
C^1(B_\delta(\theta))$, then $\sqrt n \{g(Z_n)-g(\theta)\} \to \mathcal{N}(0,
\{g'(\theta)\}^2 \sigma^2)$ in $(d)$.

\paragraph{Hypothesis testing}
$\Theta_i$ disjoint subsets of $\Theta$. $H_i: \theta \in \Theta_i$,
$i=1,2$. $H_0$ is the \emph{null hypothesis}, $H_1$ is the alternative. Test
$H_0$ against $H_1$ if we believe $\theta in \Theta_0$ or $\Theta_1$. Decide to
reject $H_0$. Data only to disprove $H_0$, lack of evidence does not mean $H_0$
is true (innocent until proven guilty). A test statistics $\psi \in {0, 1}$
such that $\psi=0$ means $H_0$ not reject, if $\psi=1$, $H_0$ rejected. Rejection region of a test $\psi$ is
\begin{align*}
  R_\psi = \{ x \in E^n: \psi = 1\}
\end{align*}
Type 1 error $\alpha_\psi: P(\psi = 1 \vert H_0)$, type 2 error $\beta_\psi =
P(\psi = 0 \vert H_1)$. Power of $\psi$: $\pi_\psi = \inf_{\theta \in \Theta_1}
\{1 - \beta_\psi(\theta)\}$. Asymptotic Level $\alpha$ if $\lim_{n\to \infty}
\alpha_{\psi_n}(\theta) \leq \alpha$, $\forall \theta \in \Theta_0$. In general
$\psi=1(T_n > c)$ for test statistic $T_n$, threshold $c\in\mathbb{R}$,
rejection region $R_\psi = \{ T_n > c \}$. $p$-value of a test $\psi_\alpha$ is
the smallest level $\alpha$ at which $\psi_\alpha$ reject $H_0$. Random and
depends on the sample. Rule: $p$-value $\leq \alpha$ iff $H_0$ is rejected
$\psi_\alpha$ at the (asymptotic) level $\alpha$.

\paragraph{Methods of estimation}
Three methods: MLE, methods of moments, M-estimators. \emph{Total variance
  distance} is $TV(P_\theta, P_\eta) = \max_{A\subset E} \vert P_\theta(A) -
P_\eta(A) \vert$ Discrete case is equal $1/2 \sum_{x\in E} \vert
p_\theta(x)-p_\eta(x) \vert$. Continuous case is $1/2 \int_E \vert
f_\theta(x)-f_\eta(x) \vert dx$. TV is a distance between probability
distribution. KL divergence is $K(P_\theta, P_\eta) = \int_E f_\theta(x) \log
\{f_\theta(x)/f_\eta(x)\} dx$ (continuous). $K(P_\theta, P_\eta) \geq 0$,
definite if zero, then args are equal. KL is a divergence, and the asymmetry is
the key to estimate it. Minimizes $KL$ is equivalent to max. $\sum_n \log
p_\theta(X_i)$. Likelihood is $L_n(x_1, \dots, x_n, \theta) = \prod_{i=1}^n
f_\theta(x_i)$. MLE is defined as
\begin{align*}
  \hat \theta_n^{MLE} = \textrm{argmax}_{\theta \in \Theta} \log\{L(X_1, \dots,
  X_n, \theta)\}.
\end{align*}
Multivariate concave functions is $x^T\textbf{H}h(\theta)x \leq 0$ for all $x
\in \mathbb{R}^d$, $\theta \in \Theta$, where $\textbf{H}h(\theta)$ is the
hessian matrix. Strictly concave with strict equality for some $\vec x \neq
\vec 0$. Optimality $\nabla h(\theta) = \vec 0$.

Bernouilli, Poisson, Gaussian mean $\bar X_n$, Gaussian variance biased sample
variance. Under regularity conditions $\hat \theta_n^{MLE} \to \theta^*$ in
$P$. $Cov(AX+B) = A\Sigma A^T$. CLT Multivariate $\sqrt{n}\Sigma^{-1/2} (\bar
X_n -\mu) \to \mathcal{N}_d(0, I_d)$ in $(d)$. Delta $\Delta$ method multivariate
$\sqrt{n}\{g(T_n) - g(\theta)\} \to \mathcal{N}_k\{0, \nabla g(\theta)^T \Sigma
\nabla g(\theta)\}$, $g: \mathbb{R}^d\to \mathbb{R}^k$.

Fischer information: $l(\theta) = \log L_1(X, \theta)$, then
\begin{align*}
  I(\theta) = - E[\textbf{H}l(\theta)] = var[l'(\theta)] = -
  E[l''(\theta)],
\end{align*}
where last two equalities only for $\theta \in \mathbb{R}$. \textbf{Don't forget the minus sign.}

\paragraph{Asymptotic normality of the MLE}
If $\theta^* \in \Theta$ (true parameter) assume the following: identifiable,
support of $P_\theta$ does not depends on $\theta^*$, $\theta^*$ is not on the
boundaries of $\Theta$, $I(\theta)$ is invertible on $B_\delta(\theta)$, and
some other technical conditions, then as $n\to \infty$
\begin{align*}
  \hat \theta_n^{MLE} & \to \theta^*, \textrm{in} \ P\\
  \sqrt n (\hat \theta_n^{MLE} - \theta^*) & \to \mathcal{N}_d\{\vec{0},
  I^{-1}(\theta)\}, \textrm{in} \ (d)
\end{align*}
w.r.t $P_{\theta^*}$.

\paragraph{Methods of Moments}
$m_k(\theta) = E_\theta[X_1^k]$. Empirical moments are the plug in averages
$\hat m_k = n^{-1} \sum_{i=1}^n g_k(X_i)$, for some differentiable (different)
$g_k$. From LLN, $\hat m_K \to m_k(\theta)$ in $P$ or a.s. Then the method of
moments identifies $\theta$ by solving $\hat\theta^{MM} =M^1(\hat m_1, \dots,
\hat m_k)$. If $M^{-1} \in C^1[B_\delta\{M(\theta)\}]$,
$\Sigma(\Theta)=Cov_\theta(X_1, \dots, X_1^d)$. Then
\begin{align*}
  \sqrt n (\hat \theta^{MM}_n - \theta) \to \mathcal{N}\{0, \Gamma(\theta)\},
  \ w.r.t \ P_\theta
\end{align*}
in $(d)$, where
\begin{align*}
\Gamma(\theta) = [\partial_\theta M^{-1}\{M(\theta)\}]^T \Sigma(\theta) [\partial_\theta M^{-1}\{M(\theta)\}]
\end{align*}
In general $MLE$ more accurate than $MM$, MLE is good if model is misspecified,
sometimes $MLE$ is intractable, but $MM$ is easier.

\paragraph{M-estimation}
Find a function $\rho: E \times \mathcal{M} \to \mathbb{R}$ where $\mathcal{M}$
is the set of all possible values of $\mu*$, such that $\mathcal{Q}(\mu)=
E\{\rho(X_1, \mu)\}$ achieves its minimum at $\mu=\mu^*$. For $\rho$ is
$L^2$-distance $\mu^*= EX$. If $\rho$ is $L^1$-distance then $\mu^*$ is the
median. If $\rho(x, \mu) = - \log L_1(x, \mu)$, then $\mu^* =
\hat\theta_n^{MLE}$.  Define $\mathcal{Q}_n(\mu)= \sum_{i=1}^n \rho(x_i,
\mu)$. If $J(\mu) = \frac{\partial^2Q}{\partial \mu \partial \mu^T}(\mu)$,
$i,j=1, \dots, n$ and $K(\mu) = Cov(\partial_\mu(X_1, \mu)$. If $\mu^* \in
\mathcal{M}$, then $\hat\mu_n \to \mu^*$ in $P$, and asymptotic distribution of
$\sqrt n (\mu^n-\mu^*)$ is centered gaussian with variance
\begin{align*}
  J(\mu^*)^{-1}K(\mu^*)J(\mu^*)^{-1}
\end{align*}

\paragraph{Hypothesis}
T-test for $\sigma^2$ unknown for gaussian, Wald's test for asymptotic
normality of $MLE$, multivariate parameters (implicit hypothesis). Goodness of
fit test. For $H_0: \mu_x = \mu_y$, $H_1 = \mu_x \neq \mu_y$. If $m=cn$ as
$n\to \infty$,
\begin{align*}
  T_{n, m} = \frac{\bar X_n - \bar Y_m - (\mu_x - \mu_y)}{\sqrt{\hat \sigma_x^2/n + \hat \sigma_y^2/m}} \to \mathcal{N}(0, 1)
\end{align*}
where $\sigma_x^2$ and $\sigma_y^2$ are the unbiased estimator of variance of
$X$ and $Y$. Rejection region is of type ${T_{n, m} > q_\alpha}$. One sided,
two sample test. If $n$, and $m$ are small, we can't apply Slutsky.

The $\chi^2$ distribution with $d > 0$ degree of freedom is the law of
$\sum_{i=1}^d Z_i$, where $Z_i$ are iid standard Gaussian. $\chi^2_2 =
Exp(1/2)$. PDF is $\Gamma(d/2, 1/2)$. Properties for $V \sim \chi^2_d$. $EV =
d$, $var(V) = 2d$. If $S_n$ is the biased sample variance, then $\bar X_n \perp
S_n$ for all $n$, and $nS_n/\sigma^2 \sim \chi^2_{n-1}$.

For $d \in \mathbb{N}^*$, the Student's $T$ distribution with $d$ d.o.f $t_d$
is the law of the random variable $Z/\sqrt{V/d}$ where $Z \sim \mathcal{N}(0,
1)$ and $V \sim \chi^2_d$ and $Z \perp V$. If $T_n = \sqrt n \bar X_n/\sqrt{
\tilde S_n}$, then $T_n \sim $t_{n-1}$. Student's $T$ test with non asymptotic
level $\alpha$ is of type $\psi_\alpha = 1(\vert T_n \vert >
q_{\alpha/2})$. One sided we have $\psi_\alpha = 1(T_n > q_{\alpha})$. For two
sample $T-test$, Test statistics is the same as with normality, but the degree
of freedom is defined by Welch-Satterhwaite formula.
\begin{align*}
  N = \Big(\frac{\sigma_x^4}{n^2(n-1)} + \frac{\sigma_y^4}{m^2(m-1)}\Big)^{-1}(\sigma_x^2/n + \sigma_y^2/m)^2
\end{align*}
Advantage of $T$-test exact test, drawback of popluation is gaussian.

Test on MLE. If $\theta \in \mathbb{R}^d$, $H_0: \theta = \theta_0$, $H_1:
\theta \neq \theta_0$, then under $H_0$,
$\sqrt{n}I(\hat\theta_n^{MLE})^{1/2}(\hat\theta_n^{MLE} - \theta_0) \to
\mathcal{N}_d(0, 1)$ in $(d)$. Hence by squaring we get in $(d)$
\begin{align*}
  n (\hat\theta_n^{MLE} - \theta_0)^T I(\hat\theta_n^{MLE})(\hat\theta_n^{MLE} - \theta_0) \to \chi^2_d
\end{align*}
The left side of the equation is denoted as $T_n$. The Wald's test with
asymptotic level $\alpha$ is $\psi = 1(T_n > q_\alpha)$ with $q_\alpha$ is the
$1-\alpha$ quantile of $\chi_d^2$. Wald test also valid if $H_1$ is one sided,
but less powerful.

Test on log likelihood: for checking if models with $d$ parameters can be
assumed, we can use Wilk's Theorem. If we assume the big model has $r$ more
dimension than the smaller model, $\theta^{MLE}_n$ is the MLE and $\theta_n^c$ is
the constrained MLE the smaller model, then
\begin{align*}
  T_n = 2\{l_n(\theta_n) - l_n(\theta_n^c)\} \to \chi^2_{d-r}
\end{align*}
in $(d)$. Where $l(\cdot)$ is the log-likelhood as a function of
$\theta$. The test is then $\psi = 1(T_n > q_\alpha)$ with $q_\alpha$ the
$(1-\alpha)$ quantiles of $\chi^2_{d-r}$.

Implicit hypotheses: if $\theta \in \mathbb{R}^d$, and $g: \mathbb{R}^d \to
\mathbb{R}^k$, $k<d$, $g\in C^1(\mathbb{R})$. If $H_0: g(\theta) = 0$, $H_1:
g(\theta) \neq 0$. Then $\Delta$ method and assuming $g(\theta) = 0$ we get
\begin{align*}
  T_n = n g(\hat\theta_n) \Gamma^{-1}(\hat\theta_n) g(\hat \theta_n) \to \chi^2_k,
\end{align*}
in $(d)$, where $\Gamma(\theta) = \nabla g(\theta)^T \Sigma(\theta) \nabla
g(\theta) \in \mathbb{R}^{k\times k}$. Check if $T_n > q_\alpha$ for $q_\alpha$
being the $1-\alpha$ quantile of $\chi^2_k$.

Goodness of fit: for discrete case, with $K$ distinct values,
\begin{align*}
  T_n = n \sum_{j=1}^K (\hat p_j - p^0_j)^2/p^0_j \to \chi^2_{\textbf{K-1}}
\end{align*}
Use the $T_n$ and $\chi^2_{K-1}$ to make the test and $p$-values.

Empirical CDF: $F_n(t) = 1/n \sum_{i=1}^n 1(X_i \leq t)$. Glivenko-Cantelli
Theorem $\sum_{t\in \mathbb{R}} \vert F_n(t) - F(t) \vert \to 0$ a.s. By CLT,
$F_n(t)$ converge $F(t)$ with variance $F(t) (1-F(t))$. Donsker theorem
provides the distribution of of Glivenko-Cantelli statistics (supremum brownian
bridge). If $H_0: F = F^0$, and $H_1: F \neq F^0$. then with Kolmogorv-Smirnov
let $T_n = \sup_{t \in \mathbb{R}} \sqrt n \vert F_n(t) - F^0(t)\vert$, then use a
pivot and check the KS test. Otherwise check the $Q-Q$ plots.

\paragraph{LM}
The regression function $f(x) = E[Y\vert X= x]$ is the regression function. The least square estimator is
\begin{align*}
  \hat \beta = \textrm{arg}\min_{\beta \in \mathbb{R}^p} \sum_{i=1}^n (Y_i - X_i^T\beta)^2
\end{align*}
For statistical inference, the design matrix is deterministic with rank $p$,
noise are iid gaussian with same unknown variance. $\hat\sigma^2 = (n-p)^{-1}
\vert\vert Y - X\hat\beta\vert\vert^2_2$. Theorems: $(n-p) \hat \sigma^2/\sigma^2
\sim \chi^2_{n-p}$, and $\hat \beta \perp \hat\sigma^2$. Significance test, for
$H_0: \beta_j = 0$ vs $H_1: \beta_j = 0$, if $\gamma_j$ is the $j$-th diagnoial coefficient of $(X^TX)^{-1}$, then
\begin{align*}
  T_n^{(j)} = \frac{\hat \beta_j - \beta_j}{\sqrt{\hat \sigma^2 \gamma_j}} \sim t_{n-p}
\end{align*}
Test with non asymptotic level $\alpha$, check $\vert T_n^{(j)} \vert >
q_{\alpha/2}(t_{n-p})$, where $q_{\alpha/2}(t_{n-p})$ for $1-\alpha/2$ quantile
of $t_{n-p}$.

\paragraph{GLM}
Generalized linear models have two components $Y$ continuous r.v with $Y \vert
X = x $ following some distribution with mean $\mu(x) = g^{-1}(x^T\beta)$, the
regression function, $g$ is the link function. For poisson $g(x) =
x^{-1}$. Exponential family: density has the form
\begin{align*}
  f_\theta(y) = h(y) \exp[<\vec{\eta}(\theta), \vec{T}(y)> - B(\theta)].
\end{align*}

For gaussian, if $\mu, \sigma^2$ is unkown, then $\theta = (\mu/\sigma^2,
-1/2\sigma^2)$, $T(y) = (y, y^2)$, $B(\theta) = \mu^2/(2\sigma^2)+\log(\sigma
\sqrt{2\pi}$. If only $\mu$ is unkown, $\eta = (\mu/\sigma^2)$, $T(y) = y$,
$B(\theta) = \mu^2/2\sigma^2$, $h(y) =
\exp(y^2/2\sigma^2)/\sqrt{2\pi\sigma^2}$. Poisson, Bernouilli, Gamma, Inverse
Gamma, Inverse Gaussian, $\chi^2$, Beta, Binomial, negative binomial are also
exponential. For $y\in \mathbb{R}$ and $k=1$
\begin{align*}
  f_\theta(y) = \exp\Big\{\frac{y\theta - b(\theta)}{\phi} + c(y, \phi)\Big\}
\end{align*}
If $\phi$ the dispersion parameter is known, then $\theta$ is the only
canonical parameter, if unkown, may/may not be a two parameter exp. family. For
Gaussian with known $\sigma^2$, $\theta=\mu$, $\phi=\sigma^2$,
$b(\theta)=\theta^2/2$. For poisson, $b(\theta)=e^\theta^$, bernouilli
$b(\theta) = \log(1 + e^\theta)$. For gamma, $b(\theta)=-\log(-\theta)$. For
exp. family, $E[\partial_\theta l] = 0$,
$E[\partial^2_{\theta^2} l] + E[\{\partial_\theta l\}^2] = 0$, leading to
$E[Y] = b'(\theta)$, and $Var(Y) = b''(\theta)\phi$. The link function has to
be bijective and $C^1$. For discrete, link map $\mathbb{R}_+^*$ to
$\mathbb{R}$. For Bernouilli, logit function is
$\log[\mu(x)/\{1-\mu(X)]=X^T\beta$, or probit with inverse Gaussian cdf as
link. Canonical link $g(\mu) = \theta$. Since $\mu = b'(\theta)$. Then
$g = (b')^{-1}$. Using the canonical link, we have for a random sample
\begin{align*}
  l_n(y, X, \beta) = \sum_{i=1}^n \frac{Y_iX_i^T\beta - b(X^T\beta)}{\phi}
\end{align*}
Unique MLE if using the canonical link function.

\end{multicols*}
#+end_export
