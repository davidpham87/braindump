#+TITLE: Micromaster Statistics and Data Science
#+OPTIONS: toc:nil
#+ROAM_ALIAS: micromaster-statistics-and-data-science
#+ROAM_TAGS: micromaster-statistics-and-data-science msds edx probability statistics machine-learning
#+DATE: 2020-10-12
#+LATEX_CLASS: article
#+LATEX_CLASS_OPTIONS: [a4paper, 8pt]
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: \usepackage[english]{babel}
#+LATEX_HEADER: \usepackage{multicol}
#+LATEX_HEADER: \usepackage[landscape, margin=0.5cm]{geometry}
#+OPTIONS: title:nil
#+OPTIONS: author:nil
#+OPTIONS: date:nil

* Probability and Statistics

\footnotesize

#+begin_export latex
\begin{multicols*}{4}

\paragraph{Partitions}
Given $n$ elements, and $k_i \in \mathbb{N}, i=1, \dots, r$ with $\sum_i k_i = n$, the number
of partition of the set into $r$ disjoint subset with the $i^{th}$ subset
containing exactly $k_i$ elements is equal to
\begin{align*}
  \binom{n}{k_1 \cdots k_r} = \frac{n!}{k_1!\dots k_r!}.
\end{align*}

\paragraph{Transformation}
Let $X$ be a continuous r.v and $g$ monotonoic when $f_X(x) > 0$ then for
$Y=g(X)$, $f_y(y) = f_X(h(y))\vert \partial_yh(y)\vert$, where $h = g^-1$ where $g$
is monotonic. If $Z=X+Y$ then $f_z(z) = \int f_X(x)f_Y(z-x) dz$.

\paragraph{Expectation}
\begin{align*}
Eg(\vec X) & = \sum_{\vec x} g(\vec x) p_{\vec X}(\vec x), \quad g: \mathbb{R}^n \to \mathbb{R} \\
Eg(\vec X) & = \int_{\mathbb{R}^n} g(\vec x) f_{\vec X}(\vec x) d\vec x
\end{align*}

\paragraph{Dependence}
$Cov(X,Y)$ is bilinear, and $Cov(X,Y) = E[(X-\mu)(Y-\nu)] = EXY -
EXEY$. $\rho_{X,Y}=Cov(x,y)/\sigma_x\sigma_y$ The total variance is
$Var(Y) = E[Var(Y\vert X)] + Var(E[Y\vert X])$. If $N$ random,
$X_1, \dots, X_N$ are iid, then
\begin{align*}
  EY & = EN \cdot EX \\
  Var(Y) & = EN \cdot Var(X) + (EX)^2 Var(N)
\end{align*}

Iterated expectetation $E[E[X \vert Y] = E[X]$. $E[X\vert Y]$ is the rv that
take value $E[X \vert Y = y]$ whenever $Y=y$.

\paragraph{Conditional PMF}
\begin{align*}
p_{X\vert Y}(x \vert y) & = \frac{p_{X,Y}(x,y)}{p_Y(y)} \\
f_{X\vert Y}(x \vert y) & = \frac{f_{X,Y}{X,Y}(x,y)}{f_Y(y)}
\end{align*}

\paragraph{Markov-chain}
\emph{recurrent} (come back), \emph{transient} (never come back),
GCD of number of steps for returning is $ >1 $, otherwise \emph{aperiodic}.
\emph{Transition matrix} $Q^1$, where $q_{ij} = P(X_{n+1}=i\vert X_n =
i)$. $Q^m$ gives the probability for $m$ steps. If $X_0 \sim \vec p$, then
$X_n \sim \vec{p} Q^n$. \emph{Stationary} is $\vec sQ =  \vec s$.


\paragraph{Inequalities}
\begin{enumerate}
\item $\vert EXY \vert^2 \leq EX^2 EY^2$ (Cauchy-Schwarz)
\item $P(X \geq a) \leq E\vert X \vert / a$, $a> 0$. (Markov)
\item $P(\vert X-\mu \vert \geq a) \leq \sigma^2/a^2$. (Chebyshev)
\item $g(EX) \leq Eg(X)$ if $g$ convex, reverse if $g$ concave. (Jensen)
\end{enumerate}
\paragraph{Convergence}
$(Y_n)_{n=1}^\infty \to Y$ in probability if
$P(\vert Y_n - Y \vert < \epsilon) \to 1$, $n \to \infty$,
$\forall \epsilon > 0$, $Y$ a rv. \emph{Slutsky}, if $X_n \to a$, $\Y_n \to b$
in prob. (constants), then $X_n+Y_n \to a+b$, if $g\in C^0(\mathbb{R})$,
$g(X_n) \to g(a)$. $EX_n$ does not always converge to $a$.

WLLN. $(X_i)_{i=1}^\infty$ i.i.d, $EX_i = \mu$,
$X \in L^2(\mathbb{R})$, then $\forall \epsilon > 0$
\begin{align*}
\lim_{n\to\infty} P(\vert \bar X_n - \mu \vert < \epsilon) = 1
\end{align*}
CLT. Same assumptions as wlln. Then $Var(X_i) = \sigma^2$, and
$Z_n = \sum_{i=1}^n (X_i-\mu)/(\sigma\sqrt n)$, then
\begin{align*}
\lim_{n\to\infty} F_{Z_n}(z)\to F_z(z), \quad Z \sim \mathcal{N}(0, 1).
\end{align*}
De Moivre-Laplace Approximation to Binomial
\begin{align*}
  P(X=i) = P\Big(i-\frac{1}{2} \leq X \leq i + \frac{1}{2}\Big)
\end{align*}
using CLT to approximate the PMF of $X$.

\paragraph{Laws}

Bernouilli: $p(k)=\binon{n}{k} p^k (1-p)^{n-k}$, $EX=p$, $Var(X)=np(1-p)$.

Geometric: $p(k) = (1-p)^{k-1}p$, $EX = p^{-1}$, $Var(X)= (1-p)/p^2$.  Let $n$
be a given a time and let $T$ be the first time of success after $n$. Then
$T-n$ follows a geometric distribution with parameter $p$ and
$T - n \perp X_1, \dots,X_n$. Let $Y_k = \sum_i=1^k T_k$, then $EY_k=k/p$,
$Var(Y_k)=kp/(1-p)^2$. PMF is Pascal PMF of order $k$
\begin{align*}
  p_{Y_k}(t) = \binom{t-1}{k-1}p^k (1-p)^{t-k}, \quad t=k,k+1, \dots
\end{align*}

Poisson: $p(k) = e^{-\lambda} \lambda^k/k!$, $k=0,1,\dots$. $EX=\lambda$,
$Var(X)=\lambda$. For a fixed lambda, binomial law converge to poisson with
$p=\lambda/n$. So poisson is a good approx if $\lambda = np$, $n$ large and $p$
really small.

Poisson process $P(k, \tau)$ ($k$ arrivals, intervals length $\tau$):
time-homogeneity, independence, small interval probabilities (probabilities are
$O(\tau)$.

Exponential: $f(t) = \lambda \exp(-\lambda t)$, $EX = 1/\lambda$,
$Var(X)=1/\lambda^2$.

Poisson is indpendent of paste, waiting time is exponential and indpendent of
past. $k^{th}$ arrival time is described as the sum of each arrival time and
has $EY_k=k/\lambda$ and $Var(Y_k) = k/\lambda^2$. PDF is
$f(y) = \lambda^k y^{k-1} e^{-\lambda y}/(k-1)!$.

\paragraph{Sum of RV}
If $Y = X_1 + \dots + X_N$, then
\begin{enumerate}
\item $X_i \sim Ber(p)$, $N \sim Bin(m, q)$, $Y\sim Bin(m, pq)$.
\item $X_i \sim Ber(p)$, $N \sim Poi(\lambda)$, $Y\sim Poi(\lambda p)$.
\item $X_i \sim Geom(p)$, $N \sim Geom(q)$, $Y\sim Geom(pq)$.
\item $X_i \sim Exp(\lambda)$, $N \sim Geom(q)$, $Y\sim Exp(\lambda q)$.
\end{enumerate}

When two process are poissons, the distribution of the combination of both
event is a poisson where rates are added ($\lambda = \sum_i r_i$).. Expectation
time between two events is twice the mean. Each event in a component has a
probability $r_i/\lambda$.

\paragraph{Bayesian Inference} Maximum a posteriori prob. (MAP), Least mean
squares (LMS), Linear least mean squares (OLS). Bayesian inference:
\begin{enumerate}
\item start with a prior $f_\Theta$ of $\Theta$.
\item have a model $f_{X\vert \Theta}$ of the observation $X$.
\item Update $p_{\Theta\vert x}$ using Baye's rule.
\end{enumerate}
Update rule (adapt for discrete laws).
\begin{align*}
  f_{\Theta \vert X}(\theta \vert x) =
  \frac{f_\Theta(\theta) f_{X \vert \Theta}(x \vert \theta)}
  {\int f_\Theta(\eta) f_{X \vert \Theta}(x \vert \eta) d\eta}
\end{align*}
MAP:
$\hat\theta = \textrm{argmax}_\theta f_\Theta(\theta)f_{X\vert \Theta}(x\vert
\theta)$ ($\hat\theta$ maximizes the posterior). If $\Theta$ is discrete, the
MAP minimizes (over all decision rules) the prob. of selecting an incorrect
hypothesis. Estimator is a function of the data.  Conditional Expectation (LMS)
sets the $\hat \theta = E[\Theta \vert X = x]$. Hypothesis testing: MAP rules
selects hypothesis which has the largest a posteriori distribution. The MAP
rule minimizes the probability of selecting an incorrect hypothesis for any
observation value x, as well as the probability of error over all decision
rules.

Estimators: $\hat\theta = E\Theta$ minimizes the equation $E(\Theta-\theta)^2$
(idem for $E[\dot \vert X = x]$). $\hat g(X) = E[\Theta \vert X]$ minimizes
$E(\Theta - g(X))^2$ over all estimators. $\hat g(X)$ is unbiased [0
(un)conditional mean]. The error $\tilde \Theta = \hat \Theta - \theta $
is uncorrelated with the estimates. The variance of $\Theta$ can be decomposed as
$Var(\hat \Theta) + Var(\tilde \Theta)$. The linear LMS is given by
\begin{align*}
  \hat \Theta = E \Theta + \frac{cov(\Theta, X)}{var(X)} (X - EX)
\end{align*}
with mse equal to $(1- \rho^2)\sigma_\Theta^2 $,
$\rho = cov(\Theta, X)/(\sigma_\Theta\sigma_X)$.


\paragraph{Classical Statistical Inferance}
Estimator error $\tilde \Theta = \hat \Theta - \theta $, bias
$E \tilde \Theta $. Expected value, variance and bias depends on $\hat \Theta$
while estimation error also $X_1 \dots, X_n$. Unbiased if bias is 0 for all
$\theta$, asymptotically unbiased if $E\hat \Theta_n \to \theta$, for all
$\theta$. An estimator is consistent if the sequence $\hat \Theta_n$ converge
to the true parameter $\theta$ for all possible $\theta$.

MLE $\hat \theta = argmax h(f_X(x \vert \theta))$, where $h$ is bijective function
(e.g. $\log$). Under some condition MLE is consistent and asymptotically
normal. $\bar X_n$ is unbiased for $\mu$ and variance $\sigma^2/n$. Variance
estimator $\hat S^2_n = \frac{1}{n-1} \sum_{i=1}^n (X_i - \bar X_n)^2$ is
unbiased.

Confidence Interval of $1-\alpha$ such that
\begin{align*}
P(\Theta \in [\hat \Theta^-_n, \hat \Theta^+_n])) \geq 1-\alpha, \quad \forall \textrm{ valid } \theta.
\end{align*}
OLS: $\beta_1 = \sigma_{xy}/\sigma_x$, $\beta_0 = \bar y - \beta_1 \bar x$,
with $\sigma_x$ begin the biased estimator of the standard deviation. Bayesian LS assume
\begin{itemize}
\item $Y_i = \Theta_0 + \Theta_1 x_i + W_i$
\item $x_i$ are known constants, $\Theta_0, \Theta_1, W_j$ are normal independent.
\item $E\Theta_0 = E\Theta_1 = 0$ and variance $\Theta_i = \sigma_i^2$, $i=1,2$, $W_j \sim \mathcal{N}(0, \sigma^2)$.
\end{itemize}

Estimates are

\begin{align*}
  \hat \theta_1 & = \frac{\sigma_1^2}{\sigma^2+\sigma_1^2\sum_i (x_i - \bar x)Â²}
  \sum_{i=1}^n (x_i -\bar x) (y_i - \bar y) \\
  \hat \theta_0 & = \frac{n\sigma_0^2}{\sigma^2+n\sigma_0^2} (\bar y - \hat \theta_1 \bar x)
\end{align*}

Likelihood ratio test: start with a target value $\alpha$ (5\%) for false
rejection prob. Choose $\xi$ such that $P(L(X)>\xi \vert H_0) = \alpha$. Once
the value $x$ of X is observed, reject $H_0$ is $L(x) > \xi$. Neyman-Person
Lemma, given $\xi$, we have $P(L(X)>\xi \vert H_0) = \alpha$ and
$P(L(X)>\xi \vert H_1) = \beta$. Suppose that some other test, with rejection
region $R$, achieves a smaller or equal false rejection prob:
$P(X\in R \vert H_0) \leq a$. Then $P(X \notin R \vert H_1) \geq \beta$, with
strict inequality, when $P(X \in R \vert H_0) < \alpha$.

Significance testing method: choose a test statistic, find the shape of
rejection region given $H_0$, choose the significance level, and the critical
value $\xi$ so that prob. of false rejection is around $\alpha$. This sets the
rejection region. Reject hypothesis $H_0$ if the observed test statistics falls
in the rejection region.


\end{multicols*}
#+end_export
